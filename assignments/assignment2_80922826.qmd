---
title: "Assignment 2"
author: "Eryk Halicki (80922826)"
date: last-modified
format:
  html:
    embed-resources: true
    df-print: tibble
    html-math-method: mathml
execute:
  echo: true
---

Since the assingment states we will use "logistic regression, K-Nearest Neighbors (KNN), and discriminant analysis to build predictive models" I believe this is A (Classification) and C(Supervised). This is because logistic regression, LDA / QDA, and KNN are supervised classification methods, since they all require labelled data. Furthermore, we are predicting a binary value (churn), indicating classification.

```{r}
telco <- read.csv('/Users/erykhalicki/Documents/School/UBC/year3/DATA311/datasets/Telco-Customer-Churn.csv')

# Binary categorical columns -> factor
telco[c("gender", "Partner", "Dependents", "PhoneService", "PaperlessBilling", "Churn")] <- 
  lapply(telco[c("gender", "Partner", "Dependents", "PhoneService", "PaperlessBilling", "Churn")], factor)

# SeniorCitizen is coded as 0/1 but should be factor
telco$SeniorCitizen <- factor(telco$SeniorCitizen, levels = c(0, 1), labels = c("No", "Yes"))

# Unordered categorical with 3+ levels
telco[c("MultipleLines", "InternetService", "OnlineSecurity", "OnlineBackup", 
        "DeviceProtection", "TechSupport", "StreamingTV", "StreamingMovies", "PaymentMethod")] <- 
  lapply(telco[c("MultipleLines", "InternetService", "OnlineSecurity", "OnlineBackup", 
                 "DeviceProtection", "TechSupport", "StreamingTV", "StreamingMovies", "PaymentMethod")], factor)

# Ordered categorical
telco$Contract <- factor(telco$Contract, 
                         levels = c("Month-to-month", "One year", "Two year"),
                         ordered = TRUE)

# customerID stays as character (it's an identifier)
# tenure, MonthlyCharges, TotalCharges stay numeric
cleaned_telco <- na.omit(telco)
str(telco)
```
Our dataset has `r nrow(telco)` rows before cleaning and `r ncol(telco)` columns. `r nrow(telco) - nrow(cleaned_telco)` rows contain NA values and will be removed.

```{r}
#| label: fig-barplot
#| fig-cap:
#|     - Churn Count
telco <- cleaned_telco
barplot(table(telco$Churn), xlab = 'Churned', ylab = 'Count', main = 'Churn Count')
```
As we can see in @fig-barplot, our classes are fairly unbalanced, with `r round(prop.table(table(telco$Churn))[1]*100, digits=2)`% of customers being in the "No" class.
Since our classes are unbalanced, models may be biased toward predicting the majority class (usually "No"). Furthermore, high accuracy can be misleading (predicting "No" for everyone gives ~`r round(prop.table(table(telco$Churn))[1]*100, digits=0)`% accuracy). To address this, we should consider using balanced evaluation metrics like precision, recall, and F1-score rather than relying solely on accuracy.

```{r}
#| label: fig-barplot-1
#| fig-width: 16
#| fig-height: 16
#| fig-cap:
#|   - Churn vs Various Factors

cat_vars <- names(telco)[sapply(telco, is.factor) & names(telco) != "Churn"]

par(mfrow = c(4, 4), cex.main = 2.0, cex.lab = 1.3, cex.axis = 1.2)

for (var in cat_vars) {
  plot(telco[[var]], telco$Churn, 
       main = paste("Churn Rate by", var),
       xlab = var, 
       ylab = "Churn",
       col = c("lightblue", "salmon"))
}

```
Looking at @fig-barplot-1 the following predictors have some relationship with churn, since there are unequal rates between of churn between levels: Senior Citizen, Partner, Dependents, Internet Service, Online Security, Online Backup, Device Protection, Tech Support, Streaming TV, Streaming Movies, Contract, Paperless Billing, and Payment Method.

```{r}
#| label: fig-barplot-2
#| fig-width: 16
#| fig-height: 16
#| fig-cap:
#|   - Churn vs Various Numerical Predictors
cont_vars <- names(telco)[sapply(telco, is.numeric)]

par(mfrow = c(2, 2))  # Adjust based on number of continuous variables

for (var in cont_vars) {
  boxplot(telco[[var]] ~ telco$Churn,
          main = paste("Churn by", var),
          xlab = "Churn",
          ylab = var,
          col = c("salmon", "lightblue"))
}
```

As we can see in @fig-barplot-2, all 3 numerical predictors have differing means and ranges across the levels of churn. This implies there is some association between their value and churn, namely, customers who churned had lower tenure, lower total charges, and higher monthly charges on average.

```{r}
set.seed(80922826)
train_indices <- sample(1:nrow(telco), size= 0.7*nrow(telco))

training_set <- telco[train_indices, ]
testing_set <- telco[-train_indices, ]
```
Our training and testing sets have `r nrow(training_set)` and `r nrow(testing_set)` rows, respectively.

```{r}
logistic_model_1 <- glm(Churn~tenure+MonthlyCharges+TotalCharges, data=training_set, family='binomial')
lm1_summary <- summary(logistic_model_1)
lm1_summary
```
Based on the `lm1_summary`, we can see how the model parameters influence our predicted log-odds. We can see that with this model fit, our intercept value of `r coef(logistic_model_1)[1]` predicts that if all other predictors are 0's, P(churn) < 0.5 (since the log-odds are below 0). The tenure coefficient of `r coef(logistic_model_1)[2]` is negative, indicating that as tenure length increases, log-odds decreases (higher chance of not churning). The last 2 coefficients, `r coef(logistic_model_1)[3]`, `r coef(logistic_model_1)[4]` for MonthlyCharges and TotalCharges, respectively, are both positive, so both correspond to increased log-odds as the predictors increase (higher chance of churn).

```{r}
logistic_model_2 <- glm(Churn~tenure+Contract+SeniorCitizen+PaymentMethod+PaperlessBilling+InternetService, data=training_set, family='binomial')
lm2_summary <- summary(logistic_model_2)
lm2_summary
```

For my model, I selected the tenure, Contract, SeniorCitizen, PaymentMethod, PaperlessBilling, and InternetService predictors. My main selection criteria was checking the bar graphs and box plots for differences in churn rate between levels of each predictor. For example, in @fig-barplot-1, we can see that the levels of SeniorCitizen (Yes and No) have noticably different rates of churn, so it was added to the model. Although there were more predictors with noticably different rates of churn between levels, I selected only the ones most noticable to me to avoid including too many features (increasing model variance).
```{r}
cols_to_fix <- c("StreamingTV", "StreamingMovies","OnlineSecurity", "OnlineBackup", "DeviceProtection", "TechSupport")

training_set[cols_to_fix] <- lapply(training_set[cols_to_fix], function(x) {
  x <- as.character(x)
  x[x == "No internet service"] <- "No"
  factor(x)
})

testing_set[cols_to_fix] <- lapply(testing_set[cols_to_fix], function(x) {
  x <- as.character(x)
  x[x == "No internet service"] <- "No"
  factor(x)
})

full_model <- glm(Churn ~ . - customerID - PhoneService,data = training_set, family = "binomial")

null_model <- glm(Churn ~ 1, data = training_set, family = "binomial")

# automatically doing forward and backward selection with our new features
backward_model <- step(full_model, direction = "backward", trace = 0)
forward_model <- step(null_model, direction = "forward", scope = formula(full_model), trace = 0)
summary(backward_model)
summary(forward_model)
```

```{r, echo=FALSE, warning=FALSE,message=FALSE}
library(class)
library(caret)
library(MASS)
```
```{r}
dummies <- dummyVars(~ tenure+Contract+SeniorCitizen+PaymentMethod+PaperlessBilling+InternetService , data = training_set) # need to use dummy variables for all factors
train_x <- predict(dummies, training_set)
head(as.data.frame(train_x))
train_y <- training_set$Churn

cross_validation_k <- 10

set.seed(80922826)
folds <- createFolds(train_y, k = cross_validation_k)

k_values <- seq(1, 100, by = 5)
cv_errors <- numeric(length(k_values))

for (i in 1:length(k_values)) {
  knn_k <- k_values[i]
  fold_errors <- numeric(cross_validation_k)
  
  for (fold in 1:cross_validation_k) {
    val_indices <- folds[[fold]]
    fold_train_x <- train_x[-val_indices, ]
    fold_train_y <- train_y[-val_indices]
    fold_val_x <- train_x[val_indices, ]
    fold_val_y <- train_y[val_indices]
    
    knn_pred <- knn(train = fold_train_x, test = fold_val_x, cl = fold_train_y, k = knn_k)
    
    fold_errors[fold] <- 1-mean(knn_pred == fold_val_y)
  }
  
  cv_errors[i] <- mean(fold_errors)
}

best_knn_k <- k_values[which.min(cv_errors)]
plot(k_values, cv_errors, type='b')
```

From our above cross validation, we can see that our best k value is `r best_knn_k`, with values between 30-60 all having similar performance.

```{r}
set.seed(80922826)
test_dummies <- dummyVars(~ tenure+Contract+SeniorCitizen+PaymentMethod+PaperlessBilling+InternetService , data = testing_set) 
test_x <- predict(dummies, testing_set)
test_y <- testing_set$Churn
test_knn_preds <- knn(train = train_x, test = test_x, cl = train_y, k = best_knn_k)
test_error <- 1-mean(test_knn_preds == test_y)
```

Our final KNN model acheives a test error of `r test_error`.

Setting a reproducible seed could be important for the KNN algorithm for 3 main reasons: training set split, cross validation set split, and tie-breaking. Firstly, our random seed determines what subset of our data will in the training set. This directly determines what points the KNN algorithm uses during prediction. Also, the random sampling in our cross-validation will impact the value of K we choose, influencing our model. The other part of the algorithm impacted by our seed would be tie breaking. In the case where a prediction is a tie between multiple classes, we need to randomly break the tie, which introduces non-determinism into our prediction, which is influenced by the random seed.


```{r}
lda_model <- lda(Churn ~ tenure + TotalCharges, data = training_set)
qda_model <- qda(Churn ~ tenure + TotalCharges, data = training_set)
```

## Model Evaluation

```{r logistic-model-1-eval} 
# take all trained models, run inference on test set
# calculate confusion matrix
# recall = TP / (TP + FN), predicted actual churn / (predicted actual churn + missed churn)
actual_churn <- testing_set$Churn
logistic_model_1_logits <- predict(logistic_model_1, newdata = testing_set)# if logits > 0, P(churn | x) > 0.5, so we predict churn
logistic_model_1_preds <- factor(logistic_model_1_logits>0, levels= c(FALSE, TRUE), labels=c("No", "Yes"))
logistic_model_1_TP <- length(which(logistic_model_1_preds == "Yes" & actual_churn == "Yes"))
logistic_model_1_FN <- length(which(logistic_model_1_preds != "Yes" & actual_churn == "Yes"))
logistic_model_1_recall <- logistic_model_1_TP / (logistic_model_1_TP + logistic_model_1_FN)
table(logistic_model_1_preds, actual_churn)
```

```{r logistic-model-2-eval} 
logistic_model_2_logits <- predict(logistic_model_2, newdata = testing_set) #returns logits
logistic_model_2_preds <- factor(logistic_model_2_logits>0, levels= c(FALSE, TRUE), labels=c("No", "Yes"))# if logits > 0, P(churn | x) > 0.5, so we predict churn
logistic_model_2_TP <- length(which(logistic_model_2_preds == "Yes" & actual_churn == "Yes"))
logistic_model_2_FN <- length(which(logistic_model_2_preds != "Yes" & actual_churn == "Yes"))
logistic_model_2_recall <- logistic_model_2_TP / (logistic_model_2_TP + logistic_model_2_FN)
table(logistic_model_2_preds, actual_churn)
```

*Note: Instructions do not mention to evaluate backward and forward selection models from exercise 10, but I've opted to evaluate them out of curiosity.*
```{r logistic-model-forward-eval} 
logistic_model_forward_logits <- predict(forward_model, newdata = testing_set) #returns logits
logistic_model_forward_preds <- factor(logistic_model_forward_logits>0, levels= c(FALSE, TRUE), labels=c("No", "Yes"))# if logits > 0, P(churn | x) > 0.5, so we predict churn
logistic_model_forward_TP <- length(which(logistic_model_forward_preds == "Yes" & actual_churn == "Yes"))
logistic_model_forward_FN <- length(which(logistic_model_forward_preds != "Yes" & actual_churn == "Yes"))
logistic_model_forward_recall <- logistic_model_forward_TP / (logistic_model_forward_TP + logistic_model_forward_FN)
table(logistic_model_forward_preds, actual_churn)
```

```{r logistic-model-backward-eval} 
logistic_model_backward_logits <- predict(backward_model, newdata = testing_set) #returns logits
logistic_model_backward_preds <- factor(logistic_model_backward_logits>0, levels= c(FALSE, TRUE), labels=c("No", "Yes"))# if logits > 0, P(churn | x) > 0.5, so we predict churn
logistic_model_backward_TP <- length(which(logistic_model_backward_preds == "Yes" & actual_churn == "Yes"))
logistic_model_backward_FN <- length(which(logistic_model_backward_preds != "Yes" & actual_churn == "Yes"))
logistic_model_backward_recall <- logistic_model_backward_TP / (logistic_model_backward_TP + logistic_model_backward_FN)
table(logistic_model_backward_preds, actual_churn)
```

```{r knn-eval} 
dummies <- dummyVars(~ tenure+Contract+SeniorCitizen+PaymentMethod+PaperlessBilling+InternetService , data = training_set) # need to use dummy variables for all factors
test_dummies <- dummyVars(~ tenure+Contract+SeniorCitizen+PaymentMethod+PaperlessBilling+InternetService , data = testing_set) # need to use dummy variables for all factors
train_x <- predict(dummies, training_set)
train_y <- training_set$Churn
test_x <- predict(dummies, testing_set)
test_y <- actual_churn
test_knn_preds <- knn(train = train_x, test = test_x, cl = train_y, k = best_knn_k)
knn_TP <- length(which(test_knn_preds == "Yes" & actual_churn == "Yes"))
knn_FN <- length(which(test_knn_preds != "Yes" & actual_churn == "Yes"))
knn_recall <- knn_TP / (knn_TP + knn_FN)
table(test_knn_preds, actual_churn)
```

```{r lda-eval} 
lda_preds <- predict(lda_model, newdata = testing_set)$class
lda_TP <- length(which(lda_preds == "Yes" & actual_churn == "Yes"))
lda_FN <- length(which(lda_preds != "Yes" & actual_churn == "Yes"))
lda_recall <- lda_TP / (lda_TP + lda_FN)
table(lda_preds, actual_churn)
```

```{r qda-eval} 
qda_preds <- predict(qda_model, newdata = testing_set)$class
qda_TP <- length(which(qda_preds == "Yes" & actual_churn == "Yes"))
qda_FN <- length(which(qda_preds != "Yes" & actual_churn == "Yes"))
qda_recall <- qda_TP / (qda_TP + qda_FN)
table(qda_preds, actual_churn)
```

```r
model_list <- c("Basic Logistic Regression", 
                "My Logistic Regression Model", 
                "Backward Stepwise Logistic Regression", 
                "Forward Stepwise Logistic Regression", 
                "KNN model",
                "LDA",
                "QDA")

model_recalls <- c(logistic_model_1_recall,
                   logistic_model_2_recall,
                   logistic_model_backward_recall,
                   logistic_model_forward_recall,
                   knn_recall,
                   lda_recall,
                   qda_recall)

model_recall_df <- data.frame(Model = model_list, 
                               Recall = model_recalls)
```
: Model Recall Comparison on Test Set {#tbl-recall}

| Model | Recall (on test set) |
|-------|----------------------|
| Basic Logistic Regression | `r logistic_model_1_recall` |
| My Logistic Regression Model | `r logistic_model_2_recall` |
| Backward Stepwise Logistic Regression | `r logistic_model_backward_recall` |
| Forward Stepwise Logistic Regression | `r logistic_model_forward_recall` |
| KNN model | `r knn_recall` |
| LDA | `r lda_recall` |
| QDA | `r qda_recall` |

As we can see in @tbl-recall, our QDA model has the highest recall of all models at `r qda_recall`.

### Bonus improved model
Since the QDA model performed the best, I'll be fitting a new QDA model using a few predictors that were significant in our forward and backward selection models. 
Although this doesnt guarentee the QDA model will improve, I beleive its a decent feature selection method.

```{r}
# Create dummy variables
dummies <- dummyVars(~ tenure+TotalCharges+PaperlessBilling+Contract, data = training_set)
test_dummies <- dummyVars(~ tenure+TotalCharges+PaperlessBilling+Contract, data = testing_set)

train_x <- predict(dummies, training_set)
train_y <- training_set$Churn
test_x <- predict(test_dummies, testing_set)
train_qda_data <- as.data.frame(train_x)
train_qda_data$Churn <- train_y
test_qda_data <- as.data.frame(test_x)

head(test_qda_data)

better_qda_model <- qda(Churn ~ tenure+TotalCharges+PaperlessBilling.Yes+Contract.L, data = train_qda_data)

better_qda_preds <- predict(better_qda_model, newdata = test_qda_data)$class
better_qda_TP <- length(which(better_qda_preds == "Yes" & actual_churn == "Yes"))
better_qda_FN <- length(which(better_qda_preds != "Yes" & actual_churn == "Yes"))
better_qda_recall <- better_qda_TP / (better_qda_TP + better_qda_FN)
table(better_qda_preds, actual_churn)
```

Although using categorical / dummy variables kind of breaks the normality assumption of the QDA model, I was able to get improved performance by including some predictors from the forward and backward fit logistic regression models, namely PaperlessBilling and Contract. This improved my QDA model performance to a recall of `r better_qda_recall`.

## Bootstrapping

```{r}
# resample test set B times with replacement
# train qda model recall for all B resamples
# evaluate recall on Out of Bag samples for each resample
# calculate standard deviation from all B qda model recalls
# get 95% confidence interval using 2.5 and 97.5 percentile values
B <- 1000
bootstrapped_recalls = numeric(B)
for(i in 1:B){
  train_indices <- sample(1:nrow(telco), nrow(telco), replace=TRUE)
  train_data <- telco[train_indices, ]
  test_indices <- setdiff(1:nrow(telco), unique(train_indices))
  test_data <- telco[test_indices, ]
  qda_model <- qda(Churn ~ tenure + TotalCharges, data = train_data)
  actual_churn <- test_data$Churn
  qda_preds <- predict(qda_model, newdata = test_data)$class
  qda_TP <- length(which(qda_preds == "Yes" & actual_churn == "Yes"))
  qda_FN <- length(which(qda_preds != "Yes" & actual_churn == "Yes"))
  bootstrapped_recalls[i] <- qda_TP / (qda_TP + qda_FN)
}
head(bootstrapped_recalls)
bootstrapped_recall_std_dev <- sd(bootstrapped_recalls)
bootstrapped_recall_mean <- mean(bootstrapped_recalls)
hist(bootstrapped_recalls)
recall_conf_interval <- quantile(bootstrapped_recalls, c(0.025, 0.975))
```

*I wasn't entirely sure how to approach testing the models recall using bootstrapping (should I train and test on the same boostrapped sample?), so I used the internet to try and find an answer and ended up using the OOB approach (<https://en.wikipedia.org/wiki/Out-of-bag_error>). My main reasoning was that if we train and test the model on the same bootstrapped sample, we will end up with an overfit / overly optimistic estimate at every step. However, if we retrain the model on the bootstrapped sample, and then test on the data not included in the bootstrapped sample (Out of Bag data), then we shouldn't encounter any overfitting. My other intuition was to resample from just the test set, but that didnt follow the idea of treating the original sample as the population.*

After running `r B` bootstrapped samples, we get a standard deviation of `r bootstrapped_recall_std_dev` and a mean of `r bootstrapped_recall_mean` for our recall.
We also get a 95% confidence interval of (`r recall_conf_interval[1]` - `r recall_conf_interval[2]`)
